## ğŸ“Œ AI Chatbot with Pluggable LLMs (Ollama, Mock, OpenAI-ready)


A full-stack AI chatbot built with Flask, SQLite, Docker, and Ollama, featuring persistent chat sessions, auto-generated titles, and a modern sidebar UI.


#### The application supports multiple LLM providers:


Ollama (local, free LLaMA-3)


Mock LLM (for offline development & testing)


OpenAI (planned / optional)


Designed as a hands-on project to explore LLM integration, stateful chat UX, and production-style backend architecture.


### ğŸ”— Live Demo (UI)


ğŸ‘‰ https://ai-chatbot-flask-5p5h.onrender.com/ui


### âš ï¸ Important:


The deployed demo showcases UI, sessions, titles, deletion, and persistence AI responses require a running LLM provider


Ollama â†’ local only


Mock provider â†’ works anywhere


### âœ¨ Features


ğŸ¤– AI chatbot with pluggable LLM backends


ğŸ§  Ollama (LLaMA 3) for free, local AI


ğŸ§ª Mock LLM provider for testing & offline use


ğŸ’¾ Persistent chat history using SQLite


ğŸ—‚ï¸ Sidebar with chat sessions


ğŸ·ï¸ Auto-generated chat titles


ğŸ—‘ï¸ Delete individual chats


ğŸ”„ Restore last session on refresh


 âš¡ Typing animation & smooth UX


ğŸ³ Fully Dockerized


ğŸ”Œ Environment-based LLM switching


### ğŸ§  Architecture Highlights


Clean separation of UI, storage, and LLM providers


Environment-controlled LLM selection (OLLAMA, MOCK, OPENAI)


Designed to be extensible, testable, and production-ready


No hard dependency on paid APIs


### ğŸ“‚ Project Structure

app/
 |--- routes/                    
  API routes (chat, sessions, health)
 
 |--- storage/                 
 SQLite + memory backends
 
 |--- templates/               
  HTML UI
 
 |--- static/                   
  CSS & JavaScript
 
 |--- openai_client.py        
  LLM abstraction layer
 

 

### ğŸ›  Tech Stack

#### Frontend

HTML


CSS


Vanilla JavaScript


#### Backend


Python (Flask)


SQLite


#### AI / LLM


Ollama (LLaMA 3)


Mock LLM (development/testing)


OpenAI (future-ready)


#### DevOps

Docker


Docker Compose


### ğŸ¯ Project Motivation


I built this project to:


Learn how to integrate LLMs into real applications


Design stateful chat UX with persistent sessions


Work with local LLMs instead of paid APIs


Practice clean backend architecture


Build something deployable and extensible


### âš™ï¸ LLM Providers


The active LLM provider is controlled via environment variables.


#### ğŸ§ª Mock LLM (default-safe)


Best for:


UI development


Testing


Cloud demos


LLM_PROVIDER=mock


#### ğŸ¤– Ollama (local, free)


Requires Ollama running on your machine.


LLM_PROVIDER=ollama


#### â˜ï¸ OpenAI (planned)


Included but intentionally disabled by default.


LLM_PROVIDER=openai


### ğŸš€ Getting Started (Local)

1ï¸âƒ£ Install Ollama (optional)

brew install ollama

ollama run llama3


2ï¸âƒ£ Clone the repository

git clone https://github.com/vcsodha/ai-chatbot-flask.git

cd ai-chatbot-flask


3ï¸âƒ£ Run with Docker

docker compose up --build


4ï¸âƒ£ Open in browser

http://localhost:8000/ui


### ğŸ”® Roadmap


ğŸŒ— Light / Dark themes


ğŸ–¼ï¸ Image upload support


ğŸ“¡ Streaming responses


ğŸ” Authentication


â˜ï¸ Cloud-hosted LLM support


ğŸ¤ OpenAI model toggle


### ğŸ“¸ Screenshot


Chat UI with persistent sessions ( when using mock LLM) 

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/502e6ef5-be8d-48cf-8116-3f220cae1924" />



### ğŸ§‘â€ğŸ’» Author


Vidisha Sodha


Built as a full-stack + AI learning project.
